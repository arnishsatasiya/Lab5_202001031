# -*- coding: utf-8 -*-
"""Deep_Learning_Lab1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Za2YwhX7WGqSyIr_Fuzx3ny7M6-qr_D_

# 50_Startup Dataset
"""

import numpy as np
import pandas as pd

data=pd.read_csv('/content/drive/MyDrive/Deep Learning/50_Startups.csv')
data.head()

"""# Null value checking"""

pd.isna(data["R&D Spend"])

from google.colab import drive
drive.mount('/content/drive')

pd.isna(data["Marketing Spend"])

"""# x & y

"""

y = data['Profit']
X = np.column_stack((data['R&D Spend'], data['Marketing Spend']))
from sklearn.preprocessing import StandardScaler
 
scaler = StandardScaler()
X = scaler.fit_transform(X)

y.shape, X.shape

X

"""# Gradient Decent"""

def MSE(x,y,y_hat):
  error = (y - y_hat)
  mse = (1.0 / len(x)) * np.sum(np.square(error))
  return error,mse


def gradient_descent(W, x, y):
    y_hat = x.dot(W).flatten()
    error,mse = MSE(x,y,y_hat)
    gradient = -(1.0 / len(x)) * error.dot(x)
    return gradient, mse

"""# Hyper parameter"""

learning_rate = [0.1,0.01,0.001]
gradientStop = 1e-3

"""# Parameter initialization"""

# w=np.array((-40,-40))

"""# Iteration"""

W_LIST=[]
MSE_LIST=[]
parameter=[]

for alpha in learning_rate:
  w_list=[]
  mse_list=[]
  # w=np.random.rand(2,)
  w=np.array([36000,3000])

  iterations = 1
  for i in range(10000):
      gradient, mse = gradient_descent(w, X, y)
      new_w = w - alpha * gradient

      if(iterations%10==0):
        print(f"iteration: {iterations}  mse: {mse} ")
      w_list.append(new_w)
      mse_list.append(mse)
      
  
      # Stopping Condition
      if np.sum(abs(new_w - w)) < gradientStop:
          print('Gradient Descent has converged')
          break
  
      iterations += 1
      w = new_w
  W_LIST.append(w_list)
  MSE_LIST.append(mse_list)
  parameter.append(w)

 
print('w =', parameter)

"""# Contour plot"""

W_LIST

parameter[0][0]

for i in range(3):
  levels = np.sort(np.array(MSE_LIST[i]))
  w0 = np.linspace(-parameter[i][0] * 5, parameter[i][0] * 5, 100)
  w1 = np.linspace(-parameter[i][1] * 5, parameter[i][1] * 5, 100)
  mse_vals = np.zeros(shape=(w0.size, w1.size))

  for j, value1 in enumerate(w0):
    for k, value2 in enumerate(w1):
        w_temp = np.array((value1,value2))        
        mse_vals[j, k] = gradient_descent(w_temp, X, y)[1]
  all_ws = np.array(W_LIST[i])
  plt.axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)
  plt.axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)
  for i in range(len(W_LIST[i]) - 1):
      plt.annotate('', xy=all_ws[i + 1, :], xytext=all_ws[i, :],
                  arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},
                  va='center', ha='center')
  

  CS = plt.contour(w0, w1, mse_vals, levels, linewidths=1,colors='black')
  # plt.figure(figsize=(20, 15))
  # CS=plt.contour(w0, w1, mse_vals,levels,alpha=.7)
  plt.clabel(CS, inline=1, fontsize=8)
  plt.title("Contour Plot of Gradient Descent")
  # plt.ylim([-2500,10000])
  plt.xlabel("w0")
  plt.ylabel("w1")
  plt.show()



levels[0],levels[-1]

# w0=[]
# w1=[]
# for i in w_list:
#   w0.append(i[0])
#   w1.append(i[1])

# mse_vals = np.zeros(shape=(len(w0), len(w1)))



import plotly.graph_objects as go

fig = go.Figure(data=go.Surface(
    
    z=mse_vals,
                                x=w0,
                                y=w1))

# add a countour plot
fig.update_traces(contours_z=dict(show=True, usecolormap=True,
                                  highlightcolor="limegreen", project_z=True),contours = {
        "x": {"show": True, "start": 1.5, "end": 2, "size": 0.44, "color":"white"},
        "z": {"show": True, "start": levels[0], "end": levels[-1],"color":"white"}
    })

# annotate the plot
fig.update_layout(title='Linear Model MSE Cost Surface',
                  scene=dict(
                    xaxis_title='theta_0 (intercept)',
                    yaxis_title='theta_1 (slope)',
                    zaxis_title='MSE Cost'),
                  width=700, height=700)

fig.show()

# fig = go.Figure(data =
#     go.Contour(
#         z=mse_vals,
#         x=w0,
#         y=w1,
#         colorscale='Hot',
#         contours=dict(
#             start=levels[0],
#             end=levels[-1],
#             # size=2,
#         ),
#     ))

# fig.show()



import matplotlib.pyplot as plt

plt.axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)
plt.axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)
for i in range(len(w_list) - 1):
    plt.annotate('', xy=all_ws[i + 1, :], xytext=all_ws[i, :],
                 arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},
                 va='center', ha='center')
 

# CS = plt.contour(w0, w1, mse_vals, levels, linewidths=1,colors='black')
plt.figure(figsize=(20, 15))
CS=plt.contour(w0, w1, mse_vals,levels,alpha=.7)
plt.clabel(CS, inline=1, fontsize=8)
plt.title("Contour Plot of Gradient Descent")
plt.ylim([-2500,10000])
plt.xlabel("w0")
plt.ylabel("w1")
plt.show()

"""# User Dataset"""

import numpy as np 
from numpy import log,dot,exp,shape
import matplotlib.pyplot as plt
import pandas as pd

data=pd.read_csv('/content/drive/MyDrive/Deep Learning/User_Data.csv')

y = data['Purchased']
X = np.column_stack((data['Age'], data['EstimatedSalary']))

from sklearn.preprocessing import StandardScaler
 
scaler = StandardScaler()
X = scaler.fit_transform(X)

X

X.shape, y.shape

def sigmoid(z):
    return 1.0/(1 + np.exp(-z))

def loss(y, y_hat):
    loss = -np.mean(y*(np.log(y_hat)) - (1-y)*np.log(1-y_hat))
    return loss

def gradient_descent(w,X, y):
    
    # X --> Input.
    # y --> true/target value.
    # y_hat --> hypothesis/predictions.
    # w --> weights (parameter).
    
    # m-> number of training examples.
    y_hat=sigmoid(np.dot(X, w))
    m = X.shape[0]
    
    # Gradient of loss w.r.t weights.
    dw = (1/m)*np.dot(X.T, (y_hat - y))
    error=loss(y,y_hat)
  
    
    return dw,error

alpha = .1
gradientStop = 1e-3
w=np.random.rand(2,)

w_list=[]
error_list=[]

iterations = 1
for i in range(2000):
    gradient,error = gradient_descent(w, X, y)
    new_w = w - alpha * gradient

    print(f"iteration: {iterations}  error: {error} ")
    w_list.append(new_w)
    error_list.append(error)
    
 
    # Stopping Condition
    if np.sum(abs(new_w - w)) < gradientStop:
        print('Gradient Descent has converged')
        break
 
    iterations += 1
    w = new_w
 
print('w =', w)

levels = np.sort(np.array(error_list))

w0 = np.linspace(-w[0] * 5, w[0] * 5, 100)
w1 = np.linspace(-w[1] * 5, w[1] * 5, 100)
error_vals = np.zeros(shape=(w0.size, w1.size))

for i, value1 in enumerate(w0):
    for j, value2 in enumerate(w1):
        w_temp = np.array((value1,value2))        
        error_vals[i, j] = gradient_descent(w_temp, X, y)[1]

all_ws = np.array(w_list)

import matplotlib.pyplot as plt
plt.figure(figsize=(20, 15))
plt.axhline(0, color='black', alpha=.5, dashes=[2, 4],linewidth=1)
plt.axvline(0, color='black', alpha=0.5, dashes=[2, 4],linewidth=1)
for i in range(len(w_list) - 1):
    plt.annotate('', xy=all_ws[i + 1, :], xytext=all_ws[i, :],
                 arrowprops={'arrowstyle': '->', 'color': 'r', 'lw': 1},
                 va='center', ha='center')
 

CS = plt.contour(w0, w1, error_vals, levels, linewidths=1,colors='green')

# CS=plt.contour(w0, w1, error_vals,levels,alpha=.7)
plt.clabel(CS, inline=1, fontsize=8)
plt.title("Contour Plot of Gradient Descent")
# plt.ylim([-2500,10000])
plt.xlabel("w0")
plt.ylabel("w1")
plt.show()

import plotly.graph_objects as go

fig = go.Figure(data=go.Surface(
    
    z=error_vals,
                                x=w0,
                                y=w1))

# add a countour plot
fig.update_traces(contours_z=dict(show=True, usecolormap=True,
                                  highlightcolor="limegreen", project_z=True),contours = {
        "x": {"show": True, "start": 1.5, "end": 2, "size": 0.44, "color":"white"},
        "z": {"show": True, "start": levels[0], "end": levels[-1],"color":"white"}
    })

# annotate the plot
fig.update_layout(title='Linear Model MSE Cost Surface',
                  scene=dict(
                    xaxis_title='theta_0 (intercept)',
                    yaxis_title='theta_1 (slope)',
                    zaxis_title='MSE Cost'),
                  width=700, height=700)

fig.show()

